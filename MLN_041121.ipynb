{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural network OOP.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladgap/Various/blob/main/MLN_041121.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYWGdV-GBZJ5"
      },
      "source": [
        "Perceptron -- neuron with an activation function.\n",
        "Inputs and their weights are numbered 0 to(n-1), bias is n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWfs6xKyIiBC"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUCN7lc5sieC"
      },
      "source": [
        "# Single perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSEiZemKj3xT"
      },
      "source": [
        "class Neuron:\n",
        "    \"\"\"A single neuron with an activation function: \"linear\" by default, \"sigmoid\".\n",
        "       Attributes:\n",
        "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
        "          bias:   The bias term. By defaul it's 1.0.\"\"\"\n",
        "\n",
        "    def __init__(self, inputs, bias = 1.0, activ = 'linear'):\n",
        "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias).\"\"\" \n",
        "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
        "        self.bias = bias\n",
        "        self.activ = activ\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
        "        sum = np.dot(np.append(x,self.bias),self.weights)\n",
        "        if self.activ == 'linear':\n",
        "          return sum\n",
        "        if self.activ == 'sigmoid':\n",
        "          return self.sigmoid(sum)\n",
        "\n",
        "    def set_weights(self, w_init):\n",
        "        \"\"\"Overrides the np.random.rand() weights and the bias weight\"\"\"\n",
        "        # w_init is a list of floats. Organize it as you'd like.\n",
        "        self.weights=np.array(w_init)\n",
        "\n",
        "    def set_activ(self, activ):\n",
        "        self.activ = activ\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # return the output of the sigmoid function applied to x\n",
        "        return 1/(1+np.exp(-x))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN9wRG3NlI4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f961204a-e5cc-46a5-a88e-602fb249a5ca"
      },
      "source": [
        "neu=Neuron(inputs=2)\n",
        "neu.set_weights([10,10,-15]) \n",
        "# neu.set_activ('sigmoid')\n",
        "neu.run([1,1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk3BV9L-wU2d"
      },
      "source": [
        "class Perceptron:\n",
        "    \"\"\"A single neuron with the sigmoid activation function.\n",
        "       Attributes:\n",
        "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
        "          bias:   The bias term. By defaul it's 1.0.\"\"\"\n",
        "\n",
        "    def __init__(self, inputs, bias = 1.0):\n",
        "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias).\"\"\" \n",
        "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
        "        self.bias = bias\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
        "        sum = np.dot(np.append(x,self.bias),self.weights)\n",
        "        return self.sigmoid(sum)\n",
        "\n",
        "    def set_weights(self, w_init):\n",
        "        \"\"\"Overrides the np.random.rand() weights and the bias weight\"\"\"\n",
        "        # w_init is a list of floats. Organize it as you'd like.\n",
        "        self.weights=np.array(w_init)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # return the output of the sigmoid function applied to x\n",
        "        return 1/(1+np.exp(-x))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo32Xuop0eYO"
      },
      "source": [
        "## AND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLvN9yxHGsw",
        "outputId": "7d18bea5-6d07-46d3-99e5-8caeb2ddf210"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([10,10,-15]) #AND gate\n",
        "\n",
        "print(\"Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gate:\n",
            "0 0 = 0.0000003059\n",
            "0 1 = 0.0066928509\n",
            "1 0 = 0.0066928509\n",
            "1 1 = 0.9933071491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6fkDspqDcz"
      },
      "source": [
        "#proverka\n",
        "# a=[]\n",
        "# a.append(Perceptron(inputs=2))\n",
        "# a[1].weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiUtQWL2qFC"
      },
      "source": [
        "## OR gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJy3bJjS2qhO",
        "outputId": "26cdc189-7490-481a-b367-7c1925e6363c"
      },
      "source": [
        "neuron = Perceptron(inputs=2)\n",
        "neuron.set_weights([10,10,-5]) #OR gate\n",
        "\n",
        "print(\"Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gate:\n",
            "0 0 = 0.0066928509\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.9999996941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CyK0jOA4vGH"
      },
      "source": [
        "## NAND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH_wkvCq4vpB",
        "outputId": "6d603dea-4e1c-44fd-914c-d9598f18cff6"
      },
      "source": [
        "neuron = Perceptron(inputs=2)\n",
        "neuron.set_weights([-10,-10,15]) #NAND gate\n",
        "\n",
        "print(\"Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gate:\n",
            "0 0 = 0.9999996941\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.0066928509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rpjT9eO6LeS"
      },
      "source": [
        "# Multilayer perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My-snyukphuZ"
      },
      "source": [
        "class MultiLayerNeuron:     \n",
        "    \"\"\"A multilayer neuron class that uses the Neuron class above.\n",
        "       Builds a list of neurons.\n",
        "       Attributes:\n",
        "          layers:  A python list with the number of elements (incl bias) per layer. Incl the input layer.\n",
        "          bias:    The bias term. The same bias is used for all neurons.\n",
        "          eta:     The learning rate.\"\"\"\n",
        "\n",
        "    def __init__(self, layers, bias = 1.0, eta = 0.5):\n",
        "        \"\"\"Return a new MLP object with the specified parameters.\"\"\" \n",
        "        self.layers = np.array(layers,dtype=object)\n",
        "        self.bias = bias\n",
        "        self.eta = eta\n",
        "        self.network = [] # The list of lists of neurons (perceptrons).\n",
        "        self.values = []  # The list of lists of neurons' (perceptrons') output values.\n",
        "        self.d = []       # The list of lists of error terms (lowercase deltas)\n",
        "\n",
        "        # 2 nested loops to create neurons layer by layer\n",
        "        for i in range(len(self.layers)): # outer loop iterates on each layer\n",
        "            self.values.append([]) #The new list of values will be filled with zeros, for every neuron in the layer. \n",
        "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
        "            self.d.append([])\n",
        "            self.d[i] = [0.0 for j in range(self.layers[i])]                        \n",
        "            self.network.append([])\n",
        "            if i > 0:      #network[0] is the input layer, so it has no neurons\n",
        "                for j in range(self.layers[i]): # inner loop iterates on each neuron in a layer\n",
        "                    percep=Neuron(inputs = self.layers[i-1], bias = self.bias) # \n",
        "                    self.network[i].append(percep) # adding j perceptrons\n",
        "        \n",
        "        self.network = np.array([np.array(x) for x in self.network],dtype=object) #transforms list of lists to numpy array\n",
        "        self.values = np.array([np.array(x) for x in self.values],dtype=object)\n",
        "        self.d = np.array([np.array(x) for x in self.d],dtype=object)\n",
        "\n",
        "    def set_weights(self, w_init): # set_weights of the MultiLayer class\n",
        "        \"\"\"Set the weights. \n",
        "           w_init is a list of lists with the weights for all but the input layer.\"\"\"\n",
        "        for i in range(len(w_init)):\n",
        "            for j in range(len(w_init[i])):\n",
        "                self.network[i+1][j].set_weights(w_init[i][j]) # set_weights for each perceptron i\n",
        "\n",
        "    def printWeights(self):\n",
        "        print()\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                print(\"Layer\",i+1,\"Neuron\",j,self.network[i][j].activ,self.network[i][j].weights)\n",
        "        print()\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Feed a sample x into the MultiLayer Perceptron.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        self.values[0] = x\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):  \n",
        "                self.values[i][j] = self.network[i][j].run(self.values[i-1]) #runs preceptrons with the previous outputs\n",
        "        return self.values[-1]\n",
        "\n",
        "    def bp(self, x, y):\n",
        "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        outputs = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        error = (y - outputs)\n",
        "        MSE = sum( error ** 2) / self.layers[-1]\n",
        "        # STEP 3: Calculate the output error terms\n",
        "        self.d[-1] = outputs * (1 - outputs) * (error)\n",
        "        # STEP 4: Calculate the error term of each unit on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = self.values[i][h] * (1-self.values[i][h]) * fwd_error\n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    if k==self.layers[i-1]:\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k]\n",
        "                    self.network[i][j].weights[k] += delta\n",
        "        return MSE"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2dcROXuqJT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4393d8b3-80ea-46e8-9ae7-f64b96927db6"
      },
      "source": [
        "#test code\n",
        "mlp1 = MultiLayerNeuron(layers=[2,2,1])  #mlp\n",
        "mlp1.set_weights([[[-10,-10,15],[15,15,-10]],[[10,10,-15]]])\n",
        "mlp1.printWeights()\n",
        "print(\"MLP:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mlp1.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mlp1.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mlp1.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mlp1.run([1,1])[0]))\n",
        "print (\"1 1 =\",mlp1.run([1,1]))\n",
        "\n",
        "print ()\n",
        "print (mlp1.network[1][0].weights) # network is list of lists of perceptrons. Each has attribute \"weights\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 2 Neuron 0 linear [-10 -10  15]\n",
            "Layer 2 Neuron 1 linear [ 15  15 -10]\n",
            "Layer 3 Neuron 0 linear [ 10  10 -15]\n",
            "\n",
            "MLP:\n",
            "0 0 = 35.0000000000\n",
            "0 1 = 85.0000000000\n",
            "1 0 = 85.0000000000\n",
            "1 1 = 135.0000000000\n",
            "1 1 = [135.]\n",
            "\n",
            "[-10 -10  15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm6nhLnw3ssI"
      },
      "source": [
        "class MultiLayerPerceptron:     \n",
        "    \"\"\"A multilayer perceptron class that uses the Perceptron class above.\n",
        "       Builds a list of perceptrons.\n",
        "       Attributes:\n",
        "          layers:  A python list with the number of elements (incl bias) per layer. Incl the input layer.\n",
        "          bias:    The bias term. The same bias is used for all neurons.\n",
        "          eta:     The learning rate.\"\"\"\n",
        "\n",
        "    def __init__(self, layers, bias = 1.0, eta = 0.5):\n",
        "        \"\"\"Return a new MLP object with the specified parameters.\"\"\" \n",
        "        self.layers = np.array(layers,dtype=object)\n",
        "        self.bias = bias\n",
        "        self.eta = eta\n",
        "        self.network = [] # The list of lists of neurons (perceptrons).\n",
        "        self.values = []  # The list of lists of neurons' (perceptrons') output values.\n",
        "        self.d = []       # The list of lists of error terms (lowercase deltas)\n",
        "\n",
        "        # 2 nested loops to create neurons layer by layer\n",
        "        for i in range(len(self.layers)): # outer loop iterates on each layer\n",
        "            self.values.append([]) #The new list of values will be filled with zeros, for every neuron in the layer. \n",
        "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
        "            self.d.append([])\n",
        "            self.d[i] = [0.0 for j in range(self.layers[i])]                        \n",
        "            self.network.append([])\n",
        "            if i > 0:      #network[0] is the input layer, so it has no neurons\n",
        "                for j in range(self.layers[i]): # inner loop iterates on each neuron in a layer\n",
        "                    percep=Perceptron(inputs = self.layers[i-1], bias = self.bias) # \n",
        "                    self.network[i].append(percep) # adding j perceptrons\n",
        "        \n",
        "        self.network = np.array([np.array(x) for x in self.network],dtype=object) #transforms list of lists to numpy array\n",
        "        self.values = np.array([np.array(x) for x in self.values],dtype=object)\n",
        "        self.d = np.array([np.array(x) for x in self.d],dtype=object)\n",
        "\n",
        "    def set_weights(self, w_init): # set_weights of the MultiLayer class\n",
        "        \"\"\"Set the weights. \n",
        "           w_init is a list of lists with the weights for all but the input layer.\"\"\"\n",
        "        for i in range(len(w_init)):\n",
        "            for j in range(len(w_init[i])):\n",
        "                self.network[i+1][j].set_weights(w_init[i][j]) # set_weights for each perceptron i\n",
        "\n",
        "    def printWeights(self):\n",
        "        print()\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                print(\"Layer\",i+1,\"Neuron\",j,self.network[i][j].weights)\n",
        "        print()\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Feed a sample x into the MultiLayer Perceptron.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        self.values[0] = x\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):  \n",
        "                self.values[i][j] = self.network[i][j].run(self.values[i-1]) #runs preceptrons with the previous outputs\n",
        "        return self.values[-1]\n",
        "\n",
        "    def bp(self, x, y):\n",
        "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        outputs = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        error = (y - outputs)\n",
        "        MSE = sum( error ** 2) / self.layers[-1]\n",
        "        # STEP 3: Calculate the output error terms\n",
        "        self.d[-1] = outputs * (1 - outputs) * (error)\n",
        "        # STEP 4: Calculate the error term of each unit on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = self.values[i][h] * (1-self.values[i][h]) * fwd_error\n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    if k==self.layers[i-1]:\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k]\n",
        "                    self.network[i][j].weights[k] += delta\n",
        "        return MSE"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlSin4uPzSFX"
      },
      "source": [
        "## XOR gate=(OR+NAND)+AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-F-OPq_zSiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f3d0ae-ac26-4cd5-8ec4-9a0d5e0650b4"
      },
      "source": [
        "#test code\n",
        "mlp1 = MultiLayerPerceptron(layers=[2,2,1])  #mlp\n",
        "mlp1.set_weights([[[-10,-10,15],[15,15,-10]],[[10,10,-15]]])\n",
        "mlp1.printWeights()\n",
        "print(\"MLP:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mlp1.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mlp1.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mlp1.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mlp1.run([1,1])[0]))\n",
        "print (\"1 1 =\",mlp1.run([1,1]))\n",
        "\n",
        "print ()\n",
        "print (mlp1.network[1][0].weights) # network is list of lists of perceptrons. Each has attribute \"weights\""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 2 Neuron 0 [-10 -10  15]\n",
            "Layer 2 Neuron 1 [ 15  15 -10]\n",
            "Layer 3 Neuron 0 [ 10  10 -15]\n",
            "\n",
            "MLP:\n",
            "0 0 = 0.0066958493\n",
            "0 1 = 0.9923558642\n",
            "1 0 = 0.9923558642\n",
            "1 1 = 0.0071528098\n",
            "1 1 = [0.00715281]\n",
            "\n",
            "[-10 -10  15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZYikTxNp69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ff9502-3868-4c29-ffaf-2dff2ebe1c78"
      },
      "source": [
        "mlp2 = MultiLayerPerceptron(layers=[2,2,1])\n",
        "print(\"\\nTraining Neural Network as an XOR Gate...\\n\")\n",
        "for i in range(5000):\n",
        "    MSE = 0.0\n",
        "    MSE += mlp2.bp([0,0],[0])\n",
        "    MSE += mlp2.bp([0,1],[1])\n",
        "    MSE += mlp2.bp([1,0],[1])\n",
        "    MSE += mlp2.bp([1,1],[0])\n",
        "    MSE = MSE / 4\n",
        "    if(i%200 == 0):\n",
        "        print (MSE)\n",
        "\n",
        "mlp2.printWeights()\n",
        "    \n",
        "print(\"MLP:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mlp2.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mlp2.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mlp2.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mlp2.run([1,1])[0]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Neural Network as an XOR Gate...\n",
            "\n",
            "0.2641516880115423\n",
            "0.26082119290114153\n",
            "0.2599024388912923\n",
            "0.25916288630209916\n",
            "0.25726277812681975\n",
            "0.24169258579748676\n",
            "0.17924143014205796\n",
            "0.1492578783179129\n",
            "0.1403811749434996\n",
            "0.13666415410787608\n",
            "0.13469975878070936\n",
            "0.13350489613787142\n",
            "0.13270843374425428\n",
            "0.13214256498537758\n",
            "0.13172126252754035\n",
            "0.13139618264913291\n",
            "0.13113819192735396\n",
            "0.13092874278757616\n",
            "0.13075549280812776\n",
            "0.13060992420782713\n",
            "0.13048597676436963\n",
            "0.1303792257137964\n",
            "0.1302863677159522\n",
            "0.1302048885132302\n",
            "0.13013284173808093\n",
            "\n",
            "Layer 2 Neuron 0 [-4.22412303 -7.8401579   0.80873514]\n",
            "Layer 2 Neuron 1 [ 3.16830546 -7.72355068 -2.29885891]\n",
            "Layer 3 Neuron 0 [-5.86241087  5.22702926 -0.03095852]\n",
            "\n",
            "MLP:\n",
            "0 0 = 0.0263390193\n",
            "0 1 = 0.4910253720\n",
            "1 0 = 0.9696940234\n",
            "1 1 = 0.4936190861\n"
          ]
        }
      ]
    }
  ]
}