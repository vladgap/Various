{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural network OOP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBW+5yUbc51CfziozQxbxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladgap/Various/blob/main/neural_network_OOP_241021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYWGdV-GBZJ5"
      },
      "source": [
        "Perceptron -- neuron with an activation function.\n",
        "Inputs and their weights are numbered 0 to(n-1), bias is n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWfs6xKyIiBC"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk3BV9L-wU2d"
      },
      "source": [
        "class Perceptron:\n",
        "    \"\"\"A single neuron with the sigmoid activation function.\n",
        "       Attributes:\n",
        "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
        "          bias:   The bias term. By defaul it's 1.0.\"\"\"\n",
        "\n",
        "    def __init__(self, inputs, bias = 1.0):\n",
        "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias).\"\"\" \n",
        "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
        "        self.bias = bias\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
        "        sum = np.dot(np.append(x,self.bias),self.weights)\n",
        "        return self.sigmoid(sum)\n",
        "\n",
        "    def set_weights(self, w_init):\n",
        "        \"\"\"Overrides the np.random.rand() weights and the bias weight\"\"\"\n",
        "        # w_init is a list of floats. Organize it as you'd like.\n",
        "        self.weights=np.array(w_init)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # return the output of the sigmoid function applied to x\n",
        "        return 1/(1+np.exp(-x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4L8UxS_1a3R"
      },
      "source": [
        "# Testing the perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo32Xuop0eYO"
      },
      "source": [
        "## AND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLvN9yxHGsw",
        "outputId": "9bb62997-8cb8-4af1-b541-e7ecdcece600"
      },
      "source": [
        "neuron = Perceptron(inputs=2)\n",
        "neuron.set_weights([10,10,-15]) #AND gate\n",
        "\n",
        "print(\"Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gate:\n",
            "0 0 = 0.0000003059\n",
            "0 1 = 0.0066928509\n",
            "1 0 = 0.0066928509\n",
            "1 1 = 0.9933071491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiUtQWL2qFC"
      },
      "source": [
        "## OR gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJy3bJjS2qhO",
        "outputId": "72e14a7f-8d3a-4c4b-ff7e-a1721859cc62"
      },
      "source": [
        "neuron = Perceptron(inputs=2)\n",
        "neuron.set_weights([10,10,-5]) #OR gate\n",
        "\n",
        "print(\"Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gate:\n",
            "0 0 = 0.0066928509\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.9999996941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CyK0jOA4vGH"
      },
      "source": [
        "## NAND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH_wkvCq4vpB",
        "outputId": "b87ff8ef-486b-47ce-ee44-f9d24688c77d"
      },
      "source": [
        "neuron = Perceptron(inputs=2)\n",
        "neuron.set_weights([-10,-10,15]) #NAND gate\n",
        "\n",
        "print(\"Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gate:\n",
            "0 0 = 0.9999996941\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.0066928509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rpjT9eO6LeS"
      },
      "source": [
        "# Multilayer perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2hIiKiG3sbH"
      },
      "source": [
        "## XOR gate=OR+NAND+AND.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm6nhLnw3ssI"
      },
      "source": [
        "class MultiLayerPerceptron:     \n",
        "    \"\"\"A multilayer perceptron class that uses the Perceptron class above.\n",
        "       Attributes:\n",
        "          layers:  A python list with the number of elements (incl bias) per layer. Incl the input layer.\n",
        "          bias:    The bias term. The same bias is used for all neurons.\n",
        "          eta:     The learning rate.\"\"\"\n",
        "\n",
        "    def __init__(self, layers, bias = 1.0):\n",
        "        \"\"\"Return a new MLP object with the specified parameters.\"\"\" \n",
        "        self.layers = np.array(layers,dtype=object)\n",
        "        self.bias = bias\n",
        "        self.network = [] # The list of lists of neurons\n",
        "        self.values = []  # The list of lists of neurons' output values        \n",
        "        \n",
        "        # 2 nested loops to create neurons layer by layer\n",
        "        for i in range(len(self.layers)): # outer loop iterates on each layer\n",
        "            self.values.append([]) #The new list of values will be filled with zeros, for every neuron in the layer. \n",
        "            self.network.append([])\n",
        "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
        "            if i > 0:      #network[0] is the input layer, so it has no neurons\n",
        "                for j in range(self.layers[i]): # inner loop iterates on each neuron in a layer\n",
        "                    self.network[i].append(Perceptron(inputs = self.layers[i-1], bias = self.bias))\n",
        "        \n",
        "        self.network = np.array([np.array(x) for x in self.network],dtype=object)\n",
        "        self.values = np.array([np.array(x) for x in self.values],dtype=object)\n",
        "\n",
        "    def set_weights(self, w_init): # set_weights of the MultiLayer class\n",
        "        \"\"\"Set the weights. \n",
        "           w_init is a list of lists with the weights for all but the input layer.\"\"\"\n",
        "        for i in range(len(w_init)):\n",
        "            for j in range(len(w_init[i])):\n",
        "                self.network[i+1][j].set_weights(w_init[i][j]) # set_weights of the Perceptron class\n",
        "\n",
        "    def printWeights(self):\n",
        "        print()\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                print(\"Layer\",i+1,\"Neuron\",j,self.network[i][j].weights)\n",
        "        print()\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Feed a sample x into the MultiLayer Perceptron.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        self.values[0] = x\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):  \n",
        "                self.values[i][j] = self.network[i][j].run(self.values[i-1])\n",
        "        return self.values[-1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlSin4uPzSFX"
      },
      "source": [
        "### XOR gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-F-OPq_zSiK",
        "outputId": "f9a3627a-a07d-4602-af85-7d23f3c6ac35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#test code\n",
        "mlp = MultiLayerPerceptron(layers=[2,2,1])  #mlp\n",
        "mlp.set_weights([[[-10,-10,15],[15,15,-10]],[[10,10,-15]]])\n",
        "mlp.printWeights()\n",
        "print(\"MLP:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mlp.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mlp.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mlp.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mlp.run([1,1])[0]))\n",
        "print ()\n",
        "print (mlp.network[1][0].weights) # network adds Perceptron to each layer. It has attribute \"weights\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 2 Neuron 0 [-10 -10  15]\n",
            "Layer 2 Neuron 1 [ 15  15 -10]\n",
            "Layer 3 Neuron 0 [ 10  10 -15]\n",
            "\n",
            "MLP:\n",
            "0 0 = 0.0066958493\n",
            "0 1 = 0.9923558642\n",
            "1 0 = 0.9923558642\n",
            "1 1 = 0.0071528098\n",
            "\n",
            "[-10 -10  15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZFL_rd9ygi"
      },
      "source": [
        ""
      ]
    }
  ]
}