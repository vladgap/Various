{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladgap/Various/blob/ver1/MLN-041121.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYWGdV-GBZJ5"
      },
      "source": [
        "Perceptron -- neuron with an activation function.\n",
        "Inputs and their weights are numbered 0 to(n-1), bias is n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWfs6xKyIiBC"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUCN7lc5sieC"
      },
      "source": [
        "# Single Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSEiZemKj3xT"
      },
      "source": [
        "class Neuron:\n",
        "    \"\"\"A single neuron with an activation function.\n",
        "       Attributes:\n",
        "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
        "          bias:   The bias term. By defaul it's 1.0.\n",
        "          activ:  The activation function: linear (default), relu, sigmoid.\"\"\"\n",
        "\n",
        "    def __init__(self, inputs, bias = 1.0, activ = 'linear'):\n",
        "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias) and random initial weights.\"\"\" \n",
        "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
        "        self.bias = bias\n",
        "        self.activ = activ\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
        "        sum = np.dot(np.append(x,self.bias),self.weights)\n",
        "        if self.activ == 'linear':\n",
        "          return sum\n",
        "        if self.activ == 'sigmoid':\n",
        "          return self.sigmoid(sum)\n",
        "        if self.activ == 'relu':\n",
        "          return self.relu(sum)\n",
        "\n",
        "    def set_weights(self, w_init):\n",
        "        \"\"\"Overrides the np.random.rand() weights and the bias weight.\"\"\"\n",
        "        # w_init is a list of floats. Organize it as you'd like.\n",
        "        self.weights=np.array(w_init)\n",
        "\n",
        "    def set_activ(self, activ):\n",
        "        \"\"\"Overrides the 'linear' activation function.\"\"\"\n",
        "        self.activ = activ\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # return the output of the sigmoid function applied to x\n",
        "        return 1/(1+np.exp(-x))\n",
        "    \n",
        "    def relu(self, x):\n",
        "        if x >= 0:\n",
        "          return x\n",
        "        return -x"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN9wRG3NlI4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91f7cf4-c716-44e3-ba7a-beed7b9d2629"
      },
      "source": [
        "neu=Neuron(inputs=2)\n",
        "neu.set_weights([10,10,-15]) \n",
        "# neu.set_activ('sigmoid')\n",
        "neu.run([1,1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo32Xuop0eYO"
      },
      "source": [
        "## AND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLvN9yxHGsw",
        "outputId": "9b97bfb1-ec8b-492f-f814-248a7586bf89"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([10,10,-15]) #AND gate\n",
        "\n",
        "print(\"AND Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate:\n",
            "0 0 = 0.0000003059\n",
            "0 1 = 0.0066928509\n",
            "1 0 = 0.0066928509\n",
            "1 1 = 0.9933071491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6fkDspqDcz"
      },
      "source": [
        "#proverka\n",
        "# a=[]\n",
        "# a.append(Perceptron(inputs=2))\n",
        "# a[1].weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiUtQWL2qFC"
      },
      "source": [
        "## OR gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJy3bJjS2qhO",
        "outputId": "dd761d16-2ac7-44ba-a0c3-5bc0e000b190"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([10,10,-5]) #OR gate\n",
        "\n",
        "print(\"OR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Gate:\n",
            "0 0 = 0.0066928509\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.9999996941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CyK0jOA4vGH"
      },
      "source": [
        "## NAND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH_wkvCq4vpB",
        "outputId": "bb119182-8860-44cf-d78e-4a1dd375720c"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([-10,-10,15]) #NAND gate\n",
        "\n",
        "print(\"NAND Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAND Gate:\n",
            "0 0 = 0.9999996941\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.0066928509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rpjT9eO6LeS"
      },
      "source": [
        "# Multilayer neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My-snyukphuZ"
      },
      "source": [
        "class MultiLayerNeuron:     \n",
        "    \"\"\"A multilayer neuron class that uses the Neuron class above.\n",
        "       Builds a list of neurons with the specific activation function.\n",
        "       The activation function may be modified later using the set_activ method.\n",
        "       Attributes:\n",
        "          layers:  A list with the number of neurons per layer. Including the input (first) and the output (last) layers.\n",
        "          bias:    The bias term. The same bias is used for all neurons.\n",
        "          eta:     The learning rate.\n",
        "          activ:   The activation function: linear (default), relu, sigmoid.\"\"\"\n",
        "\n",
        "    def __init__(self, layers, bias = 1.0, eta = 0.5, activ='linear'):\n",
        "        \"\"\"Return a new MLP object with the specified parameters.\"\"\" \n",
        "        self.layers = np.array(layers,dtype=object)\n",
        "        self.bias = bias\n",
        "        self.eta = eta\n",
        "        self.network = [] # The list of lists of neurons (perceptrons).\n",
        "        self.values = []  # The list of lists of neurons' (perceptrons') output values.\n",
        "        self.d = []       # The list of lists of error terms (lowercase deltas)\n",
        "        self.activ = activ\n",
        "\n",
        "        # 2 nested loops to create neurons layer by layer\n",
        "        for i in range(len(self.layers)): # outer loop iterates on each layer\n",
        "            self.values.append([]) #The new list of values will be filled with zeros, for every neuron in the layer. \n",
        "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
        "            self.d.append([])\n",
        "            self.d[i] = [0.0 for j in range(self.layers[i])]                        \n",
        "            self.network.append([])\n",
        "            if i > 0:      #network[0] is the input layer, so it has no neurons\n",
        "                for j in range(self.layers[i]): # inner loop iterates on each neuron in a layer\n",
        "                    neur=Neuron(inputs = self.layers[i-1], bias = self.bias, activ = self.activ) # \n",
        "                    self.network[i].append(neur) # adding j perceptrons\n",
        "        \n",
        "        self.network = np.array([np.array(x) for x in self.network],dtype=object) #transforms list of lists to numpy array\n",
        "        self.values = np.array([np.array(x) for x in self.values],dtype=object)\n",
        "        self.d = np.array([np.array(x) for x in self.d],dtype=object)\n",
        "\n",
        "    def set_weights(self, w_init): # set_weights of the MultiLayer class\n",
        "        \"\"\"Set the weights. \n",
        "           w_init is a list of lists with the weights for all but the input layer.\"\"\"\n",
        "        for i in range(len(w_init)):\n",
        "            for j in range(len(w_init[i])):\n",
        "                self.network[i+1][j].set_weights(w_init[i][j]) # set_weights for each perceptron i\n",
        "\n",
        "    def set_activ(self, activ):\n",
        "        \"\"\"Set the activation function to each neuron.\"\"\"\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                self.network[i][j].set_activ(activ) # set_activ for each neuron\n",
        "\n",
        "    def printWeights(self):\n",
        "        \"\"\"Displays a summary of weights and activation functions per layer and neuron.\"\"\"\n",
        "        print()\n",
        "        print('Layer 0 is the Input Layer')\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                print(\"Layer\",i,\"Neuron\",j,\":\",self.network[i][j].weights,self.network[i][j].activ)\n",
        "        print()\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Feed a sample x into the MultiLayer Neuron.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        self.values[0] = x\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):  \n",
        "                self.values[i][j] = self.network[i][j].run(self.values[i-1]) #runs preceptrons with the previous outputs\n",
        "        return self.values[-1]\n",
        "\n",
        "    def bp(self, x, y):\n",
        "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        outputs = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        error = (y - outputs)\n",
        "        MSE = sum( error ** 2) / self.layers[-1]\n",
        "        # STEP 3: Calculate the output error terms\n",
        "        self.d[-1] = outputs * (1 - outputs) * (error)\n",
        "        # STEP 4: Calculate the error term of each unit on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = self.values[i][h] * (1-self.values[i][h]) * fwd_error\n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    if k==self.layers[i-1]:\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k]\n",
        "                    self.network[i][j].weights[k] += delta\n",
        "        return MSE"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlSin4uPzSFX"
      },
      "source": [
        "## XOR gate=(OR+NAND)+AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2dcROXuqJT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c3b325-3341-4ef4-f412-aa33ccccf6c5"
      },
      "source": [
        "#test code\n",
        "mln1 = MultiLayerNeuron(layers=[2,2,1])  #mln1\n",
        "mln1.set_weights([[[-10,-10,15],[15,15,-10]],[[10,10,-15]]])\n",
        "mln1.set_activ('sigmoid') #linear is by default\n",
        "\n",
        "mln1.printWeights()\n",
        "print(\"XOR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mln1.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mln1.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mln1.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mln1.run([1,1])[0]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [-10 -10  15] sigmoid\n",
            "Layer 1 Neuron 1 : [ 15  15 -10] sigmoid\n",
            "Layer 2 Neuron 0 : [ 10  10 -15] sigmoid\n",
            "\n",
            "XOR Gate:\n",
            "0 0 = 0.0066958493\n",
            "0 1 = 0.9923558642\n",
            "1 0 = 0.9923558642\n",
            "1 1 = 0.0071528098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1cXJqv6KPD9",
        "outputId": "496a007e-b101-46b3-94bd-93762d5fdffb"
      },
      "source": [
        "print (\"1 1 =\",mln1.run([1,1]))\n",
        "print (mln1.network[1][0].weights) # network is list of lists of perceptrons. Each has attribute \"weights\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 = [0.00715281]\n",
            "[-10 -10  15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBf5fifWK_EG"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZYikTxNp69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f55c0df-0f26-4780-e7b6-767db3ad5c03"
      },
      "source": [
        "mln2 = MultiLayerNeuron(layers=[2,2,1])\n",
        "mln2.set_activ('sigmoid') #linear is by default\n",
        "print(\"\\nTraining Neural Network as an XOR Gate...\\n\")\n",
        "for i in range(5000):\n",
        "    MSE = 0.0\n",
        "    MSE += mln2.bp([0,0],[0])\n",
        "    MSE += mln2.bp([0,1],[1])\n",
        "    MSE += mln2.bp([1,0],[1])\n",
        "    MSE += mln2.bp([1,1],[0])\n",
        "    MSE = MSE / 4\n",
        "    if(i%200 == 0):\n",
        "        print (MSE)\n",
        "\n",
        "mln2.printWeights()\n",
        "    \n",
        "print(\"XOR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mln2.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mln2.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mln2.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mln2.run([1,1])[0]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Neural Network as an XOR Gate...\n",
            "\n",
            "0.2655952272543131\n",
            "0.26234668017156054\n",
            "0.2610718125536462\n",
            "0.2601974516466373\n",
            "0.2595904154386822\n",
            "0.2591410492744747\n",
            "0.2586754798794746\n",
            "0.25756811663320545\n",
            "0.2513195012782557\n",
            "0.21451242139685014\n",
            "0.18069009500624547\n",
            "0.1442378754809884\n",
            "0.04370451074031393\n",
            "0.015184739543656945\n",
            "0.008301711052972009\n",
            "0.0055427514033425645\n",
            "0.004105651052951425\n",
            "0.003237317386644974\n",
            "0.0026606916707264143\n",
            "0.0022520242698359358\n",
            "0.0019483060847153965\n",
            "0.0017142801549993622\n",
            "0.0015287617033946628\n",
            "0.001378292418398605\n",
            "0.0012539312354596635\n",
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [-4.29895347 -4.28593159  6.3559648 ] sigmoid\n",
            "Layer 1 Neuron 1 : [-6.1381838  -6.07335152  2.39688993] sigmoid\n",
            "Layer 2 Neuron 0 : [ 8.57701515 -8.78945718 -3.99753774] sigmoid\n",
            "\n",
            "XOR Gate:\n",
            "0 0 = 0.0295488375\n",
            "0 1 = 0.9677509762\n",
            "1 0 = 0.9678176270\n",
            "1 1 = 0.0405246932\n"
          ]
        }
      ]
    }
  ]
}