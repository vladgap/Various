{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladgap/Various/blob/ver1/MLN-081121_inprogress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYWGdV-GBZJ5"
      },
      "source": [
        "Perceptron -- neuron with an activation function.\n",
        "Inputs and their weights are numbered 0 to(n-1), bias is n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWfs6xKyIiBC"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUCN7lc5sieC"
      },
      "source": [
        "# Single Neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSEiZemKj3xT"
      },
      "source": [
        "class Neuron:\n",
        "    \"\"\"A single neuron with an activation function.\n",
        "       Attributes:\n",
        "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
        "          bias:   The bias term. By defaul it's 1.0.\n",
        "          activ:  The activation function: linear (default), relu, sigmoid.\"\"\"\n",
        "\n",
        "    def __init__(self, inputs, bias = 1.0, activ = 'linear'):\n",
        "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias) and random initial weights.\"\"\" \n",
        "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
        "        self.bias = bias\n",
        "        self.activ = activ\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
        "        sum = np.dot(np.append(x,self.bias),self.weights)\n",
        "        if self.activ == 'linear':\n",
        "          return sum\n",
        "        if self.activ == 'sigmoid':\n",
        "          return self.sigmoid(sum)\n",
        "        if self.activ == 'relu':\n",
        "          return self.relu(sum)\n",
        "\n",
        "    def set_weights(self, w_init):\n",
        "        \"\"\"Overrides the np.random.rand() weights and the bias weight.\"\"\"\n",
        "        # w_init is a list of floats. Organize it as you'd like.\n",
        "        self.weights=np.array(w_init)\n",
        "\n",
        "    def set_activ(self, activ):\n",
        "        \"\"\"Overrides the 'linear' activation function.\"\"\"\n",
        "        self.activ = activ\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # return the output of the sigmoid function applied to x\n",
        "        return 1/(1+np.exp(-x))\n",
        "    \n",
        "    def relu(self, x):\n",
        "        # return the output of the relu function applied to x\n",
        "        if x >= 0:\n",
        "          return x\n",
        "        return 0"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN9wRG3NlI4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b9ad68-65a7-4303-b178-ade60eb4cf0a"
      },
      "source": [
        "neu=Neuron(inputs=2)\n",
        "neu.set_weights([10,10,-15]) \n",
        "# neu.set_activ('sigmoid')\n",
        "neu.run([1,1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo32Xuop0eYO"
      },
      "source": [
        "## AND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeLvN9yxHGsw",
        "outputId": "59f40060-3e7a-426c-e474-20a764641736"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([10,10,-15]) #AND gate\n",
        "\n",
        "print(\"AND Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate:\n",
            "0 0 = 0.0000003059\n",
            "0 1 = 0.0066928509\n",
            "1 0 = 0.0066928509\n",
            "1 1 = 0.9933071491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6fkDspqDcz"
      },
      "source": [
        "#proverka\n",
        "# a=[]\n",
        "# a.append(Perceptron(inputs=2))\n",
        "# a[1].weights"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDiUtQWL2qFC"
      },
      "source": [
        "## OR gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJy3bJjS2qhO",
        "outputId": "77718fcb-bac0-4679-daff-4de9b4f0568a"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([10,10,-5]) #OR gate\n",
        "\n",
        "print(\"OR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OR Gate:\n",
            "0 0 = 0.0066928509\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.9999996941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CyK0jOA4vGH"
      },
      "source": [
        "## NAND gate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH_wkvCq4vpB",
        "outputId": "b662be44-07ec-4aee-a3e7-ad1fca262948"
      },
      "source": [
        "neuron = Neuron(inputs=2, activ='sigmoid')\n",
        "neuron.set_weights([-10,-10,15]) #NAND gate\n",
        "\n",
        "print(\"NAND Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
        "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
        "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
        "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAND Gate:\n",
            "0 0 = 0.9999996941\n",
            "0 1 = 0.9933071491\n",
            "1 0 = 0.9933071491\n",
            "1 1 = 0.0066928509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rpjT9eO6LeS"
      },
      "source": [
        "# Multilayer neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My-snyukphuZ"
      },
      "source": [
        "class MultiLayerNeuron:     \n",
        "    \"\"\"A multilayer neuron class that uses the Neuron class above.\n",
        "       Builds a list of neurons with the specific activation function.\n",
        "       The activation function may be modified later using the set_activ method.\n",
        "       Attributes:\n",
        "          layers:  A list with the number of neurons per layer. Including the input (first) and the output (last) layers.\n",
        "          bias:    The bias term. The same bias is used for all neurons.\n",
        "          eta:     The learning rate.\n",
        "          activ:   The activation function: linear (default), relu, sigmoid.\"\"\"\n",
        "\n",
        "    def __init__(self, layers, bias = 1.0, eta = 0.5, activ='linear'):\n",
        "        \"\"\"Return a new MLP object with the specified parameters.\"\"\" \n",
        "        self.layers = np.array(layers,dtype=object)\n",
        "        self.bias = bias\n",
        "        self.eta = eta\n",
        "        self.network = [] # The list of lists of neurons (perceptrons).\n",
        "        self.values = []  # The list of lists of neurons' (perceptrons') output values.\n",
        "        self.d = []       # The list of lists of error terms (lowercase deltas)\n",
        "        self.activ = activ\n",
        "\n",
        "        # 2 nested loops to create neurons layer by layer\n",
        "        for i in range(len(self.layers)): # outer loop iterates on each layer\n",
        "            self.values.append([]) #The new list of values will be filled with zeros, for every neuron in the layer. \n",
        "            self.values[i] = [0.0 for j in range(self.layers[i])]\n",
        "            self.d.append([])\n",
        "            self.d[i] = [0.0 for j in range(self.layers[i])]                        \n",
        "            self.network.append([])\n",
        "            if i > 0:      #network[0] is the input layer, so it has no neurons\n",
        "                for j in range(self.layers[i]): # inner loop iterates on each neuron in a layer\n",
        "                    neur=Neuron(inputs = self.layers[i-1], bias = self.bias, activ = self.activ) # \n",
        "                    self.network[i].append(neur) # adding j perceptrons\n",
        "        \n",
        "        self.network = np.array([np.array(x) for x in self.network],dtype=object) #transforms list of lists to numpy array\n",
        "        self.values = np.array([np.array(x) for x in self.values],dtype=object)\n",
        "        self.d = np.array([np.array(x) for x in self.d],dtype=object)\n",
        "\n",
        "    def set_weights(self, w_init): # set_weights of the MultiLayer class\n",
        "        \"\"\"Set the weights. \n",
        "           w_init is a list of lists with the weights for all but the input layer.\"\"\"\n",
        "        for i in range(len(w_init)):\n",
        "            for j in range(len(w_init[i])):\n",
        "                self.network[i+1][j].set_weights(w_init[i][j]) # set_weights for each perceptron i\n",
        "\n",
        "    def set_activ(self, activ):\n",
        "        \"\"\"Set the activation function to each neuron.\"\"\"\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                self.network[i][j].set_activ(activ) # set_activ for each neuron\n",
        "    \n",
        "    def set_output_activ(self, activ):\n",
        "        \"\"\"Set the activation function to the last (output) neuron.\"\"\"\n",
        "        i = len(self.network)-1\n",
        "        for j in range(self.layers[i]):\n",
        "            self.network[i][j].set_activ(activ) \n",
        "\n",
        "    def printWeights(self):\n",
        "        \"\"\"Displays a summary of weights and activation functions per layer and neuron.\"\"\"\n",
        "        print()\n",
        "        print('Layer 0 is the Input Layer')\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):\n",
        "                print(\"Layer\",i,\"Neuron\",j,\":\",self.network[i][j].weights,self.network[i][j].activ)\n",
        "        print()\n",
        "\n",
        "    def run(self, x):\n",
        "        \"\"\"Feed a sample x into the MultiLayer Neuron.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        self.values[0] = x\n",
        "        for i in range(1,len(self.network)):\n",
        "            for j in range(self.layers[i]):  \n",
        "                self.values[i][j] = self.network[i][j].run(self.values[i-1]) #runs preceptrons with the previous outputs\n",
        "        return self.values[-1]\n",
        "\n",
        "    def bp_classif(self, x, y):\n",
        "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm - Gradient Descent.\n",
        "        Uses the derivative of the sigmoid function.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        outputs = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        error = (y - outputs) # A list of outputs\n",
        "        MSE = sum( error ** 2) / self.layers[-1] \n",
        "        # ∂MSE/∂weight=∂MSE/∂output*∂output/∂weight\n",
        "        # STEP 3: Calculate the OUTPUT error terms\n",
        "        # ∂MSE/∂output -- depends on neuron's activation function\n",
        "        self.d[-1] = outputs * (1 - outputs) * (error) # derivative of the SIGMOID function \n",
        "        # STEP 4: Calculate the error term of EACH UNIT on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = self.values[i][h] * (1-self.values[i][h]) * fwd_error # derivative of the SIGMOID function\n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    if k==self.layers[i-1]:\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k] # applying the delta rule\n",
        "                    self.network[i][j].weights[k] += delta\n",
        "        return MSE\n",
        "\n",
        "    def bp_classif3(self, x, y):\n",
        "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm - Gradient Descent.\n",
        "        Uses the derivative of the sigmoid function.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        outputs = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        error = (y - outputs) # A list of outputs\n",
        "        MSE = sum( error ** 2) / self.layers[-1] \n",
        "        # ∂MSE/∂weight=∂MSE/∂output*∂output/∂weight\n",
        "        # STEP 3: Calculate the OUTPUT error terms\n",
        "        # ∂MSE/∂output -- depends on neuron's activation function\n",
        "        self.d[-1] = outputs * (1 - outputs) * (error) # derivative of the SIGMOID function \n",
        "        # STEP 4: Calculate the error term of EACH UNIT on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = self.deriv(self.values[i][h], i, h) * fwd_error # derivative of the SIGMOID function\n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    # output=sum(weight*value)+bias*bias_weight\n",
        "                    if k==self.layers[i-1]:\n",
        "                        # ∂output/∂bias_weight=bias\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        # ∂output/∂weight=value\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k] \n",
        "                    self.network[i][j].weights[k] += delta # applying the delta rule\n",
        "        return MSE\n",
        "\n",
        "    def deriv(self, value, i, j=1):\n",
        "        if self.network[i][j].activ == 'linear':\n",
        "          return value\n",
        "        if self.network[i][j].activ == 'sigmoid':\n",
        "          return value*(1-value)\n",
        "        if self.network[i][j].activ == 'relu':\n",
        "          if value > 0:\n",
        "            return value\n",
        "          else:\n",
        "            return 0\n",
        "\n",
        "    def bp_regres(self, x, y):\n",
        "        \"\"\"Run a single (x,y) pair with the backpropagation algorithm - Gradient Descent.\n",
        "        Uses the derivative of the linear function.\"\"\"\n",
        "        x = np.array(x,dtype=object)\n",
        "        y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        outputs = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        error = (y - outputs) # A list of outputs\n",
        "        MSE = sum( error ** 2) / self.layers[-1] \n",
        "        # STEP 3: Calculate the OUTPUT error terms\n",
        "        self.d[-1] = (error) \n",
        "        # STEP 4: Calculate the error term of EACH UNIT on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = fwd_error \n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    if k==self.layers[i-1]:\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k] # applying the delta rule\n",
        "                    self.network[i][j].weights[k] += delta\n",
        "        return MSE\n",
        "\n",
        "    def bp_regres2(self, x, y):\n",
        "        # \"\"\"Run a single (x,y) pair with the backpropagation algorithm - Gradient Descent.\n",
        "        # Uses the derivative of the linear function.\"\"\"\n",
        "        ## x = np.array(x,dtype=object)\n",
        "        ## y = np.array(y,dtype=object)\n",
        "        # STEP 1: Feed a sample to the network \n",
        "        output = self.run(x)\n",
        "        # STEP 2: Calculate the MSE\n",
        "        # error = (y - outputs) # A list of outputs\n",
        "        SE = (y[0] - output[0])**2\n",
        "        # STEP 3: Calculate the OUTPUT error terms\n",
        "        # D_SE/D_weight=D_SE/D_output*D_output/D_weight\n",
        "        # D_SE/D_output:\n",
        "        l=len(self.network) \n",
        "        m=len(self.network[l-2]) #number of neurons in a before last layer\n",
        "        self.d1[0] = -2*(y[0] - output[0])\n",
        "        # D_output/D_weight:\n",
        "        d2\n",
        "        # STEP 4: Calculate the error term of EACH UNIT on each layer\n",
        "        for i in reversed(range(1,len(self.network)-1)):\n",
        "            for h in range(len(self.network[i])):\n",
        "                fwd_error = 0.0\n",
        "                for k in range(self.layers[i+1]): \n",
        "                    fwd_error += self.network[i+1][k].weights[h] * self.d[i+1][k]               \n",
        "                self.d[i][h] = fwd_error \n",
        "        # STEPS 5 & 6: Calculate the deltas and update the weights\n",
        "        for i in range(1,len(self.network)): # runs on layers\n",
        "            for j in range(self.layers[i]): # runs on neurons\n",
        "                for k in range(self.layers[i-1]+1): # runs on inputs. +1 for bias\n",
        "                    if k==self.layers[i-1]:\n",
        "                        delta = self.eta * self.d[i][j] * self.bias\n",
        "                    else:\n",
        "                        delta = self.eta * self.d[i][j] * self.values[i-1][k] # applying the delta rule\n",
        "                    self.network[i][j].weights[k] += delta\n",
        "        return MSE"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlSin4uPzSFX"
      },
      "source": [
        "## XOR gate=(OR+NAND)+AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2dcROXuqJT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d7c67b-d20d-4379-e2ea-84df2162ed92"
      },
      "source": [
        "#test code\n",
        "mln1 = MultiLayerNeuron(layers=[2,2,1])  #mln1\n",
        "mln1.set_weights([[[-10,-10,15],[15,15,-10]],[[10,10,-15]]])\n",
        "mln1.set_activ('sigmoid') #linear is by default\n",
        "\n",
        "mln1.printWeights()\n",
        "print(\"XOR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mln1.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mln1.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mln1.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mln1.run([1,1])[0]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [-10 -10  15] sigmoid\n",
            "Layer 1 Neuron 1 : [ 15  15 -10] sigmoid\n",
            "Layer 2 Neuron 0 : [ 10  10 -15] sigmoid\n",
            "\n",
            "XOR Gate:\n",
            "0 0 = 0.0066958493\n",
            "0 1 = 0.9923558642\n",
            "1 0 = 0.9923558642\n",
            "1 1 = 0.0071528098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1cXJqv6KPD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74334e8f-e0a6-4f9b-a342-dd2517ab342d"
      },
      "source": [
        "print (\"1 1 =\",mln1.run([1,1]))\n",
        "print (mln1.network[1][0].weights) # network is list of lists of perceptrons. Each has attribute \"weights\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 = [0.00715281]\n",
            "[-10 -10  15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV8ZGDKCGdig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e5ab86-448a-4357-82fe-4bcb8daa480e"
      },
      "source": [
        "mln1=MultiLayerNeuron(layers=[2,2,1])\n",
        "mln1.set_activ('relu')\n",
        "mln1.printWeights()\n",
        "\n",
        "mln1.set_output_activ('sigmoid') # setting the output activ func\n",
        "mln1.printWeights()\n",
        "\n",
        "mln1.network[1][0].set_activ('linear') # changing specific activ func\n",
        "mln1.printWeights()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [ 0.57395121 -0.93942811 -0.80899958] relu\n",
            "Layer 1 Neuron 1 : [-0.21810499  0.73929011 -0.47119654] relu\n",
            "Layer 2 Neuron 0 : [ 0.81434658 -0.11891838  0.13320892] relu\n",
            "\n",
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [ 0.57395121 -0.93942811 -0.80899958] relu\n",
            "Layer 1 Neuron 1 : [-0.21810499  0.73929011 -0.47119654] relu\n",
            "Layer 2 Neuron 0 : [ 0.81434658 -0.11891838  0.13320892] sigmoid\n",
            "\n",
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [ 0.57395121 -0.93942811 -0.80899958] linear\n",
            "Layer 1 Neuron 1 : [-0.21810499  0.73929011 -0.47119654] relu\n",
            "Layer 2 Neuron 0 : [ 0.81434658 -0.11891838  0.13320892] sigmoid\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBf5fifWK_EG"
      },
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZYikTxNp69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc443001-da1b-4a66-c2d8-2b1248035113"
      },
      "source": [
        "mln2 = MultiLayerNeuron(layers=[2,2,1])\n",
        "mln2.set_activ('sigmoid') #linear is by default\n",
        "print(\"\\nTraining Neural Network as an XOR Gate...\\n\")\n",
        "for i in range(5000):\n",
        "    MSE = 0.0\n",
        "    MSE += mln2.bp_classif([0,0],[0])\n",
        "    MSE += mln2.bp_classif([0,1],[1])\n",
        "    MSE += mln2.bp_classif([1,0],[1])\n",
        "    MSE += mln2.bp_classif([1,1],[0])\n",
        "    MSE = MSE / 4\n",
        "    if(i%200 == 0):\n",
        "        print (MSE)\n",
        "\n",
        "mln2.printWeights()\n",
        "    \n",
        "print(\"XOR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mln2.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mln2.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mln2.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mln2.run([1,1])[0]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Neural Network as an XOR Gate...\n",
            "\n",
            "0.3448298326652236\n",
            "0.2614048111922839\n",
            "0.2603406121187662\n",
            "0.25847119242135946\n",
            "0.24642294500914724\n",
            "0.20692912008421044\n",
            "0.18523493188406853\n",
            "0.17375106125937878\n",
            "0.1287228023288545\n",
            "0.03519451504672873\n",
            "0.013441456131147448\n",
            "0.007669206177469729\n",
            "0.005230785141018132\n",
            "0.003923419604839102\n",
            "0.0031190965555119316\n",
            "0.002578364572571491\n",
            "0.002191698953691703\n",
            "0.0019023787701767266\n",
            "0.0016782581726097446\n",
            "0.001499828530992248\n",
            "0.0013545968933820758\n",
            "0.0012342088481141138\n",
            "0.0011328747014005688\n",
            "0.0010464604547826945\n",
            "0.0009719384125214212\n",
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [-6.31993106 -6.33931145  2.51179589] sigmoid\n",
            "Layer 1 Neuron 1 : [-4.37404484 -4.37567692  6.48144822] sigmoid\n",
            "Layer 2 Neuron 0 : [-8.93730261  8.78117928 -4.1117011 ] sigmoid\n",
            "\n",
            "XOR Gate:\n",
            "0 0 = 0.0263229313\n",
            "0 1 = 0.9714193693\n",
            "1 0 = 0.9713565932\n",
            "1 1 = 0.0359674838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prBVNR0eNQSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38161548-9d8d-4cde-b3dd-1b2c1d7b9f0e"
      },
      "source": [
        "mln2 = MultiLayerNeuron(layers=[2,2,1], eta=0.5)\n",
        "mln2.set_activ('sigmoid') #linear is by default\n",
        "print(\"\\nTraining Neural Network as an XOR Gate...\\n\")\n",
        "for i in range(5000):\n",
        "    MSE = 0.0\n",
        "    MSE += mln2.bp_classif3([0,0],[0])\n",
        "    MSE += mln2.bp_classif3([0,1],[1])\n",
        "    MSE += mln2.bp_classif3([1,0],[1])\n",
        "    MSE += mln2.bp_classif3([1,1],[0])\n",
        "    MSE = MSE / 4\n",
        "    if(i%200 == 0):\n",
        "        print (MSE)\n",
        "\n",
        "mln2.printWeights()\n",
        "    \n",
        "print(\"XOR Gate:\")\n",
        "print (\"0 0 = {0:.10f}\".format(mln2.run([0,0])[0]))\n",
        "print (\"0 1 = {0:.10f}\".format(mln2.run([0,1])[0]))\n",
        "print (\"1 0 = {0:.10f}\".format(mln2.run([1,0])[0]))\n",
        "print (\"1 1 = {0:.10f}\".format(mln2.run([1,1])[0]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Neural Network as an XOR Gate...\n",
            "\n",
            "0.26802568570330054\n",
            "0.2609061431909701\n",
            "0.2356007983769397\n",
            "0.1097381465523291\n",
            "0.021206170099977082\n",
            "0.009759671309357783\n",
            "0.00609333424787333\n",
            "0.004361082103940412\n",
            "0.003369064762806332\n",
            "0.0027320935724259882\n",
            "0.0022908890856233066\n",
            "0.0019683710911469338\n",
            "0.0017229359547793171\n",
            "0.0015302482440207456\n",
            "0.0013751662011900507\n",
            "0.0012477954449733528\n",
            "0.0011414078768083587\n",
            "0.001051274920450291\n",
            "0.0009739795651157502\n",
            "0.0009069932199877156\n",
            "0.0008484059218324654\n",
            "0.0007967489084537581\n",
            "0.0007508747468210182\n",
            "0.0007098744042593268\n",
            "0.0006730186605853337\n",
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [-6.03419582  6.07679964  3.09259211] sigmoid\n",
            "Layer 1 Neuron 1 : [-5.49711721  5.24732105 -2.83190303] sigmoid\n",
            "Layer 2 Neuron 0 : [-8.45593114  8.94875209  3.94292621] sigmoid\n",
            "\n",
            "XOR Gate:\n",
            "0 0 = 0.0253788708\n",
            "0 1 = 0.9759385778\n",
            "1 0 = 0.9712835740\n",
            "1 1 = 0.0225766099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsu-mr5IdZlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ef8a25-44d2-4541-ffac-e0e54e62012b"
      },
      "source": [
        "mln3 = MultiLayerNeuron(layers=[3,1], eta=0.3)\n",
        "# mln3.set_weights([[[4.9,-0.9,2.1,0.2]]])\n",
        "# mln3.printWeights()\n",
        "print(\"\\nTraining Neural Network...\\n\")\n",
        "for i in range(1000):\n",
        "    MSE = 0.0\n",
        "    MSE += mln3.bp_regres([0.7759,\t0.1104,\t0.9977,],[5.764286995])\n",
        "    MSE += mln3.bp_regres([0.9692,\t0.6961,\t0.8483,],[5.84646758])\n",
        "    MSE += mln3.bp_regres([0.0265,\t0.399,\t0.5375,],[0.808633075])\n",
        "    MSE += mln3.bp_regres([0.7694,\t0.5051,\t0.2542,],[3.850298589])\n",
        "    MSE = MSE / 4\n",
        "    if(i%200 == 0):\n",
        "        print (MSE)\n",
        "\n",
        "mln3.printWeights()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Neural Network...\n",
            "\n",
            "15.38130546122461\n",
            "4.3458165105496785e-11\n",
            "9.345460865315207e-19\n",
            "2.0092780294044934e-26\n",
            "6.902532920683853e-31\n",
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [ 4.99973369e+00 -9.99625006e-01  1.99991681e+00  3.52253295e-05] linear\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBA-VYuEJGm2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "e0642f3b-879d-459c-91ca-ea062f2c7223"
      },
      "source": [
        "mln4 = MultiLayerNeuron(layers=[3,2,1], eta=0.3)\n",
        "mln4.set_activ('sigmoid')\n",
        "mln4.set_output_activ('linear')\n",
        "mln4.set_weights([[[0,1,-2,1],[1,-2,0,1]],[[-1,2,3]]])\n",
        "mln4.printWeights()\n",
        "\n",
        "y4=[]\n",
        "y4[0]=mln4.run([2,1,0])\n",
        "y4[1]=mln4.run([-1,0,1])\n",
        "y4[2]=mln4.run([0,0,0])\n",
        "y4[3]=mln4.run([1,1,0])\n",
        "y4[4]=mln4.run([-1,-1,-1])\n",
        "y4[5]=mln4.run([-1,1,-1])\n",
        "\n",
        "i=len(mln4.network)\n",
        "len(mln4.network[i-2])\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer 0 is the Input Layer\n",
            "Layer 1 Neuron 0 : [ 0  1 -2  1] sigmoid\n",
            "Layer 1 Neuron 1 : [ 1 -2  0  1] sigmoid\n",
            "Layer 2 Neuron 0 : [-1  2  3] linear\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d5a9dd0c95b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# y4[0]=\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmln4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmln4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmln4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmln4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
          ]
        }
      ]
    }
  ]
}